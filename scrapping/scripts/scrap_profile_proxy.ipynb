{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83ed0194-fcec-4299-9ffd-cccc861119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TEST PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5ad0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom options for the Selenium driver\n",
    "options = Options()\n",
    "\n",
    "# free proxy server URL\n",
    "# proxy_server_url = \"157.245.97.60\"\n",
    "proxy_server_url = '154.6.98.26:3128'\n",
    "options.add_argument(f'--proxy-server={proxy_server_url}')\n",
    "\n",
    "# create the ChromeDriver instance with custom options\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.get('http://httpbin.org/ip')\n",
    "# driver.get('https://www.malt.fr/profile/luccharlopeau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE PROXY POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_pool = [\n",
    "    '154.6.98.26:3128'\n",
    "]\n",
    "\n",
    "# define queue to store the scraped data\n",
    "from asyncio import Queue\n",
    "\n",
    "async def append_to_queue(item):\n",
    "    await queue.put(item)\n",
    "\n",
    "async def consume_queue():\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2acab102-8494-4c6a-826b-db2e58992ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_user(link, proxy):\n",
    "    \"\"\"\n",
    "    Scrap the profile of a user on Malt.fr\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    link : list\n",
    "        link to scrap.\n",
    "    proxy : string\n",
    "        proxy to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_data : dataFrame\n",
    "        DataFrame containing all the data scrapped.\n",
    "    \"\"\"\n",
    "\n",
    "    # define custom options for the Selenium driver\n",
    "    options = Options()\n",
    "    \n",
    "    options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "    # create the ChromeDriver instance with custom options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--proxy-server=%s' % proxy)\n",
    "    # driver = uc.Chrome(options=chrome_options, use_subprocess=True)\n",
    "\n",
    "    wait = WebDriverWait(driver, 6)\n",
    "    driver.get(link)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    price_element = soup.find('div', {'data-testid': 'profile-price'})\n",
    "    if price_element:\n",
    "        price = price_element.find('span', class_='block-list__price').text\n",
    "        data['price'] = price.strip()\n",
    "    \n",
    "    # Récupérer l'expérience\n",
    "    experience_element = soup.find('span', string='Expérience')\n",
    "    if experience_element:\n",
    "        experience = experience_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['experience'] = experience.strip()\n",
    "    \n",
    "    # Récupérer le taux de réponse\n",
    "    response_rate_element = soup.find('span', string='Taux de réponse')\n",
    "    if response_rate_element:\n",
    "        response_rate = response_rate_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_rate'] = response_rate.strip()\n",
    "    \n",
    "    # Récupérer le temps de réponse\n",
    "    response_time_element = soup.find('span', string='Temps de réponse')\n",
    "    if response_time_element:\n",
    "        response_time = response_time_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_time'] = response_time.strip()\n",
    "        \n",
    "    # Récupérer le nom \n",
    "    name_element = soup.find('div', {'data-testid': 'profile-fullname'})\n",
    "    if name_element:\n",
    "        name = name_element.text\n",
    "        data['name'] = name.strip()\n",
    "        \n",
    "    # Récupérer le métier\n",
    "    headline_element = soup.find('div', {'data-testid': 'profile-headline'})\n",
    "    if headline_element:\n",
    "        headline = headline_element.text\n",
    "        data['headline'] = headline.strip()\n",
    "        \n",
    "    # Récupérer le nombre de missions\n",
    "    missions_element = soup.find('div', {'data-testid': 'profile-counter-missions'})\n",
    "    if missions_element:\n",
    "        missions = missions_element.find('strong').text\n",
    "        data['missions'] = missions.strip()\n",
    "        \n",
    "    # Récupérer toutes les catégories\n",
    "    categories_elements = soup.find_all('li', {'class': 'categories__list-item'})\n",
    "    categories = [category.find('a').text for category in categories_elements]\n",
    "    data['categories'] = categories\n",
    "    \n",
    "    \n",
    "    # Récupérer les compétences\n",
    "    competences_element = soup.find_all('div', {'class': 'profile-expertises__content-list-item__label'})\n",
    "    competences = [competence.find('a', class_='joy-link joy-link_teal').text.strip() for competence in competences_element]\n",
    "\n",
    "    data['competences'] = competences\n",
    "    \n",
    "    # Récupérer le statut \"Supermalter\"\n",
    "    supermalter_element = soup.find('span', class_='joy-badge-level__tag blue')\n",
    "    if supermalter_element:\n",
    "        supermalter = supermalter_element.get_text(strip=True)\n",
    "        data['supermalter'] = supermalter\n",
    "        \n",
    "    # Récupérer la localisation\n",
    "    location_element = soup.find('dl', {'class': 'profile__location-and-workplace-preferences__item'})\n",
    "    if location_element:\n",
    "        location_label = location_element.find('dt', {'data-testid': 'profile-location-address-label'})\n",
    "        location_value = location_element.find('dd', {'data-testid': 'profile-location-preference-address'})\n",
    "\n",
    "        if location_label and location_value:\n",
    "            location = {location_label.text: location_value.text}\n",
    "            data['location'] = location\n",
    "            \n",
    "    # Récupérer la préférence de télétravail\n",
    "    teletravail_element = soup.find('dl', {'class': 'profile-page-mission-preferences__item'})\n",
    "    if teletravail_element:\n",
    "        teletravail_label = teletravail_element.find('dt')\n",
    "        teletravail_value = teletravail_element.find('dd')\n",
    "\n",
    "        if teletravail_label and teletravail_value:\n",
    "            teletravail_preference = {teletravail_label.text: teletravail_value.text}\n",
    "            data['teletravail_preference'] = teletravail_preference\n",
    "            \n",
    "    # Récupérer le nombre de recommandations\n",
    "    recommendations_element = soup.find('span', {'data-testid': 'profile-counter-recommendations'})\n",
    "    if recommendations_element:\n",
    "        recommendations_count = int(recommendations_element.text.split()[0])\n",
    "        data['recommendations'] = recommendations_count   \n",
    "        \n",
    "\n",
    "    # Récupérer le message de présentation\n",
    "    presentation_element = soup.find('div', {'class': 'profile-description__content'})\n",
    "    if presentation_element:\n",
    "        presentation_message = presentation_element.get_text(strip=True)\n",
    "        data['presentation'] = presentation_message\n",
    "          \n",
    "    driver.quit() # close the browser\n",
    "    # time.sleep(5) # wait for 5 seconds to avoid getting banned\n",
    "    \n",
    "    return data # return the data scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_all_users_proxy(links): # links is a list of links to scrap.\n",
    "    \"\"\"\n",
    "    Scrap the profile of every link in the list of links.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    links : list\n",
    "        List of links to scrap.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_data : dataFrame\n",
    "        DataFrame containing all the data scrapped.\n",
    "    \"\"\"\n",
    "    all_data = [] # list to store all the data scrapped\n",
    "    \n",
    "    queue = Queue()\n",
    "\n",
    "    for link in links: # loop over all the links\n",
    "        scrapped_user = await scrap_user(link, proxy_pool[0]) # scrap the user\n",
    "        print(scrapped_user)\n",
    "        \n",
    "        await queue.put(scrapped_user)  # Asynchronously append to the queue\n",
    "\n",
    "\n",
    "        while not queue.empty():  # Consume the queue\n",
    "            scrapped_user = await queue.get()\n",
    "            all_data.append(scrapped_user)  # Append to the data list\n",
    "    \n",
    "    return all_data # return the list of data scrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN THE SCRIPT WITH THE LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "703537a8-6bf8-44e5-bd74-0948d0c3ddb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mawait\u001b[39;00m task\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mresult()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m main()\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mcreate_task (scrap_all_users_proxy(profile_links[:\u001b[39m3\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mawait\u001b[39;00m task\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mresult()\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m queue \u001b[39m=\u001b[39m Queue()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links: \u001b[39m# loop over all the links\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     scrapped_user \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m scrap_user(link, proxy_pool[\u001b[39m0\u001b[39m]) \u001b[39m# scrap the user\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(scrapped_user)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mawait\u001b[39;00m queue\u001b[39m.\u001b[39mput(scrapped_user)  \u001b[39m# Asynchronously append to the queue\u001b[39;00m\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mpresentation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m presentation_message\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m driver\u001b[39m.\u001b[39mquit() \u001b[39m# close the browser\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m) \u001b[39m# wait for 5 seconds to avoid getting banned\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "profile_links = pd.read_csv('../data/links.csv')\n",
    "profile_links['profil'] = profile_links['profil'].apply(lambda x: x.replace('https://www.malt.fr/profile/', ''))\n",
    "profile_links = profile_links['profil'].tolist()\n",
    "profile_links = [f'https://www.malt.fr/profile/{link}' for link in profile_links] # create the links to scrap\n",
    "\n",
    "async def main():\n",
    "    task = asyncio.create_task (scrap_all_users_proxy(profile_links[:3]))\n",
    "    await task\n",
    "    return task.result()\n",
    "data = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE THE DATA TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e4291-0e0e-47c2-bf83-2913657d7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, csv_filename):\n",
    "    # Assurez-vous que la liste de données n'est pas vide\n",
    "    if not data:\n",
    "        print(\"Aucune donnée à enregistrer.\")\n",
    "        return\n",
    "\n",
    "    # Créez une liste de noms de colonnes pour le CSV en incluant les nouveaux éléments\n",
    "    fieldnames = ['name', 'headline', 'price', 'experience', 'response_rate', 'response_time', 'missions', 'categories', 'competences', 'supermalter', 'location','presentation', 'recommendations', 'teletravail_preference']\n",
    "\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Écrire les en-têtes\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Écrire les données\n",
    "        for entry in data:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "    print(f\"Données enregistrées dans {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce171a-cfba-4045-a89e-d83c0d4af58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune donnée à enregistrer.\n"
     ]
    }
   ],
   "source": [
    "csv_filename = 'malt_data.csv'\n",
    "save_to_csv(data, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
