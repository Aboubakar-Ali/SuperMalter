{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83ed0194-fcec-4299-9ffd-cccc861119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import asyncio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TEST PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f5ad0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom options for the Selenium driver\n",
    "options = Options()\n",
    "\n",
    "# free proxy server URL\n",
    "# proxy_server_url = \"157.245.97.60\"\n",
    "options.add_argument(f'--proxy-server={proxy_pool[0]}')\n",
    "\n",
    "# create the ChromeDriver instance with custom options\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.get('http://httpbin.org/ip')\n",
    "driver.get('https://www.malt.fr/profile/luccharlopeau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE PROXY POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define queue to store the scraped data\n",
    "from asyncio import Queue\n",
    "\n",
    "async def append_to_queue(item):\n",
    "    await queue.put(item)\n",
    "\n",
    "async def consume_queue():\n",
    "    while True:\n",
    "        item = await queue.get()\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2acab102-8494-4c6a-826b-db2e58992ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_user(link, proxy):\n",
    "    \"\"\"\n",
    "    Scrap the profile of a user on Malt.fr\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    link : list\n",
    "        link to scrap.\n",
    "    proxy : string\n",
    "        proxy to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_data : dataFrame\n",
    "        DataFrame containing all the data scrapped.\n",
    "    \"\"\"\n",
    "\n",
    "    # define custom options for the Selenium driver\n",
    "    options = Options()\n",
    "    \n",
    "    options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "    # create the ChromeDriver instance with custom options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--proxy-server=%s' % proxy)\n",
    "    # driver = uc.Chrome(options=chrome_options, use_subprocess=True)\n",
    "\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "    driver.get(link)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    price_element = soup.find('div', {'data-testid': 'profile-price'})\n",
    "    if price_element:\n",
    "        price = price_element.find('span', class_='block-list__price').text\n",
    "        data['price'] = price.strip()\n",
    "    \n",
    "    # Récupérer l'expérience\n",
    "    experience_element = soup.find('span', string='Expérience')\n",
    "    if experience_element:\n",
    "        experience = experience_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['experience'] = experience.strip()\n",
    "    \n",
    "    # Récupérer le taux de réponse\n",
    "    response_rate_element = soup.find('span', string='Taux de réponse')\n",
    "    if response_rate_element:\n",
    "        response_rate = response_rate_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_rate'] = response_rate.strip()\n",
    "    \n",
    "    # Récupérer le temps de réponse\n",
    "    response_time_element = soup.find('span', string='Temps de réponse')\n",
    "    if response_time_element:\n",
    "        response_time = response_time_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_time'] = response_time.strip()\n",
    "        \n",
    "    # Récupérer le nom \n",
    "    name_element = soup.find('div', {'data-testid': 'profile-fullname'})\n",
    "    if name_element:\n",
    "        name = name_element.text\n",
    "        data['name'] = name.strip()\n",
    "        \n",
    "    # Récupérer le métier\n",
    "    headline_element = soup.find('div', {'data-testid': 'profile-headline'})\n",
    "    if headline_element:\n",
    "        headline = headline_element.text\n",
    "        data['headline'] = headline.strip()\n",
    "        \n",
    "    # Récupérer le nombre de missions\n",
    "    missions_element = soup.find('div', {'data-testid': 'profile-counter-missions'})\n",
    "    if missions_element:\n",
    "        missions = missions_element.find('strong').text\n",
    "        data['missions'] = missions.strip()\n",
    "        \n",
    "    # Récupérer toutes les catégories\n",
    "    categories_elements = soup.find_all('li', {'class': 'categories__list-item'})\n",
    "    categories = [category.find('a').text for category in categories_elements]\n",
    "    data['categories'] = categories\n",
    "    \n",
    "    \n",
    "    # Récupérer les compétences\n",
    "    competences_element = soup.find_all('div', {'class': 'profile-expertises__content-list-item__label'})\n",
    "    competences = [competence.find('a', class_='joy-link joy-link_teal').text.strip() for competence in competences_element]\n",
    "\n",
    "    data['competences'] = competences\n",
    "    \n",
    "    # Récupérer le statut \"Supermalter\"\n",
    "    supermalter_element = soup.find('span', class_='joy-badge-level__tag blue')\n",
    "    if supermalter_element:\n",
    "        supermalter = supermalter_element.get_text(strip=True)\n",
    "        data['supermalter'] = supermalter\n",
    "        \n",
    "    # Récupérer la localisation\n",
    "    location_element = soup.find('dl', {'class': 'profile__location-and-workplace-preferences__item'})\n",
    "    if location_element:\n",
    "        location_label = location_element.find('dt', {'data-testid': 'profile-location-address-label'})\n",
    "        location_value = location_element.find('dd', {'data-testid': 'profile-location-preference-address'})\n",
    "\n",
    "        if location_label and location_value:\n",
    "            location = {location_label.text: location_value.text}\n",
    "            data['location'] = location\n",
    "            \n",
    "    # Récupérer la préférence de télétravail\n",
    "    teletravail_element = soup.find('dl', {'class': 'profile-page-mission-preferences__item'})\n",
    "    if teletravail_element:\n",
    "        teletravail_label = teletravail_element.find('dt')\n",
    "        teletravail_value = teletravail_element.find('dd')\n",
    "\n",
    "        if teletravail_label and teletravail_value:\n",
    "            teletravail_preference = {teletravail_label.text: teletravail_value.text}\n",
    "            data['teletravail_preference'] = teletravail_preference\n",
    "            \n",
    "    # Récupérer le nombre de recommandations\n",
    "    recommendations_element = soup.find('span', {'data-testid': 'profile-counter-recommendations'})\n",
    "    if recommendations_element:\n",
    "        recommendations_count = int(recommendations_element.text.split()[0])\n",
    "        data['recommendations'] = recommendations_count   \n",
    "        \n",
    "\n",
    "    # Récupérer le message de présentation\n",
    "    presentation_element = soup.find('div', {'class': 'profile-description__content'})\n",
    "    if presentation_element:\n",
    "        presentation_message = presentation_element.get_text(strip=True)\n",
    "        data['presentation'] = presentation_message\n",
    "          \n",
    "    driver.quit() # close the browser\n",
    "    time.sleep(5) # wait for 5 seconds to avoid getting banned\n",
    "    \n",
    "    return data # return the data scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_all_users_proxy(links): # links is a list of links to scrap.\n",
    "    \"\"\"\n",
    "    Scrap the profile of every link in the list of links.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    links : list\n",
    "        List of links to scrap.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_data : dataFrame\n",
    "        DataFrame containing all the data scrapped.\n",
    "    \"\"\"\n",
    "    all_data = [] # list to store all the data scrapped\n",
    "    \n",
    "    queue = Queue()\n",
    "\n",
    "    index = 0\n",
    "    \n",
    "    for link in links: # loop over all the links\n",
    "        scrapped_user = await scrap_user(link, proxy_pool[0]) # scrap the user\n",
    "        print(scrapped_user)\n",
    "        index+=1\n",
    "        if index == len(proxy_pool):\n",
    "            index = 0\n",
    "        \n",
    "        all_data.append(scrapped_user)\n",
    "        \n",
    "        # NECESSARY ????\n",
    "        # await queue.put(scrapped_user)  # Asynchronously append to the queue\n",
    "\n",
    "        # while not queue.empty():  # Consume the queue\n",
    "        #     scrapped_user = await queue.get()\n",
    "        #     all_data.append(scrapped_user)  # Append to the data list\n",
    "    \n",
    "    return all_data # return the list of data scrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN THE SCRIPT WITH THE LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "703537a8-6bf8-44e5-bd74-0948d0c3ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price': '300\\xa0€', 'experience': '15 ans et +', 'response_rate': '100%', 'response_time': '12h', 'name': 'Brice Tillet', 'headline': 'Composer / Sound Designer / Music Producer', 'categories': ['Sound Designer'], 'competences': ['music production', 'Sound design', \"musique à l'image\", 'composer', 'Post production', 'musique', 'compositeur', 'musique de film', 'composition', 'illustration sonore', 'music', 'scoring', 'production', 'film scoring', 'Mixage', 'design sonore', 'designer sonore', 'illustrateur sonore', 'Montage son', 'identité sonore', 'habillage sonore', 'Avid Pro Tools', 'score'], 'location': {'Localisation': 'Paris, France'}, 'teletravail_preference': {'Peut travailler dans vos locaux à': 'Paris et 50km autour'}, 'recommendations': 6, 'presentation': \"Bonjour,je suis musicien, compositeur et sound designer depuis 2005.J'aime composer à l'image, travailler les textures sonores, jouer avec le rythme et les silences.N'hésitez pas à me contacter !Brice\"}\n",
      "{'categories': [], 'competences': []}\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_TUNNEL_CONNECTION_FAILED\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n#0 0x5648a42b16d4 <unknown>\n#1 0x5648a3fb648e <unknown>\n#2 0x5648a3faf9e9 <unknown>\n#3 0x5648a3fa2602 <unknown>\n#4 0x5648a3fa3d05 <unknown>\n#5 0x5648a3fa2b88 <unknown>\n#6 0x5648a3fa1999 <unknown>\n#7 0x5648a3fa17ff <unknown>\n#8 0x5648a3fa05ba <unknown>\n#9 0x5648a3fa0a15 <unknown>\n#10 0x5648a3fb9265 <unknown>\n#11 0x5648a4035441 <unknown>\n#12 0x5648a401e132 <unknown>\n#13 0x5648a4034f65 <unknown>\n#14 0x5648a401ded3 <unknown>\n#15 0x5648a3ff0420 <unknown>\n#16 0x5648a3ff1a93 <unknown>\n#17 0x5648a42844c0 <unknown>\n#18 0x5648a4287780 <unknown>\n#19 0x5648a42871fa <unknown>\n#20 0x5648a4287c95 <unknown>\n#21 0x5648a427665b <unknown>\n#22 0x5648a4288080 <unknown>\n#23 0x5648a4261830 <unknown>\n#24 0x5648a42a1ee7 <unknown>\n#25 0x5648a42a20f5 <unknown>\n#26 0x5648a42b0cce <unknown>\n#27 0x7f06188aa9eb <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mawait\u001b[39;00m task\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mresult()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m main()\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mcreate_task(scrap_all_users_proxy(profile_links[:\u001b[39m3\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mawait\u001b[39;00m task\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39mresult()\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links: \u001b[39m# loop over all the links\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     scrapped_user \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m scrap_user(link, proxy_pool[index]) \u001b[39m# scrap the user\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(scrapped_user)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     index\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# chrome_options = webdriver.ChromeOptions()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# chrome_options.add_argument('--proxy-server=%s' % proxy)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# driver = uc.Chrome(options=chrome_options, use_subprocess=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m wait \u001b[39m=\u001b[39m WebDriverWait(driver, \u001b[39m4\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m driver\u001b[39m.\u001b[39;49mget(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m page_source \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mpage_source\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(page_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    348\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_TUNNEL_CONNECTION_FAILED\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n#0 0x5648a42b16d4 <unknown>\n#1 0x5648a3fb648e <unknown>\n#2 0x5648a3faf9e9 <unknown>\n#3 0x5648a3fa2602 <unknown>\n#4 0x5648a3fa3d05 <unknown>\n#5 0x5648a3fa2b88 <unknown>\n#6 0x5648a3fa1999 <unknown>\n#7 0x5648a3fa17ff <unknown>\n#8 0x5648a3fa05ba <unknown>\n#9 0x5648a3fa0a15 <unknown>\n#10 0x5648a3fb9265 <unknown>\n#11 0x5648a4035441 <unknown>\n#12 0x5648a401e132 <unknown>\n#13 0x5648a4034f65 <unknown>\n#14 0x5648a401ded3 <unknown>\n#15 0x5648a3ff0420 <unknown>\n#16 0x5648a3ff1a93 <unknown>\n#17 0x5648a42844c0 <unknown>\n#18 0x5648a4287780 <unknown>\n#19 0x5648a42871fa <unknown>\n#20 0x5648a4287c95 <unknown>\n#21 0x5648a427665b <unknown>\n#22 0x5648a4288080 <unknown>\n#23 0x5648a4261830 <unknown>\n#24 0x5648a42a1ee7 <unknown>\n#25 0x5648a42a20f5 <unknown>\n#26 0x5648a42b0cce <unknown>\n#27 0x7f06188aa9eb <unknown>\n"
     ]
    }
   ],
   "source": [
    "profile_links = pd.read_csv('../data/links.csv')\n",
    "profile_links['profil'] = profile_links['profil'].apply(lambda x: x.replace('https://www.malt.fr/profile/', ''))\n",
    "profile_links = profile_links['profil'].tolist()\n",
    "profile_links = [f'https://www.malt.fr/profile/{link}' for link in profile_links] # create the links to scrap\n",
    "\n",
    "async def main():\n",
    "    task = asyncio.create_task(scrap_all_users_proxy(profile_links[:3]))\n",
    "    await task\n",
    "    return task.result()\n",
    "data = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'categories': [], 'competences': []},\n",
       " {'price': '250\\xa0€',\n",
       "  'experience': '-',\n",
       "  'response_rate': '100%',\n",
       "  'response_time': 'Quelques jours',\n",
       "  'name': 'Mickael M.',\n",
       "  'headline': 'Graphiste',\n",
       "  'categories': ['Graphiste'],\n",
       "  'competences': ['Wordpress',\n",
       "   'Design logo',\n",
       "   'Affiche',\n",
       "   'Carte de visite',\n",
       "   'Flyer',\n",
       "   'Site vitrine'],\n",
       "  'location': {'Localisation': '21000 Dijon, France'},\n",
       "  'teletravail_preference': {'Peut travailler dans vos locaux à': 'Dijon et 50km autour'},\n",
       "  'presentation': 'Passionné depuis mon enfance par le graphisme, j’ai décidé de me lancer en tant que freelance. J’ai commencé assez tôt à utiliser le logiciel Photoshop (au collège). Je passais mon temps libre à retoucher des photos, faire des montages que je mettais en ligne sur mes différents blogs.Après l’obtention de mon baccalauréat au lycée Stephen Liegeard à Brochon, je suis parti étudier le graphisme à l’Institut de l’Internet et du Multimédia au pôle universitaire Leonard de Vinci (Paris La Defense). A l’issu de mes 3 années en option communication visuelle, j’ai obtenu un bachelor chef de projet multimédia. Après mes études, je suis retourné vivre à Dijon et j’ai créé mon activité de graphiste freelance Ka Design.'},\n",
       " {'price': '520\\xa0€',\n",
       "  'experience': '8-15 ans',\n",
       "  'response_rate': '100%',\n",
       "  'response_time': '1h',\n",
       "  'name': 'Adriana Lyra',\n",
       "  'headline': 'UX/UI Designer Senior ⚡️ Brand Designer',\n",
       "  'missions': '21',\n",
       "  'categories': ['UI Designer', 'UX Designer'],\n",
       "  'competences': ['UX/UI design',\n",
       "   'User Interface Design',\n",
       "   'User Experience Design',\n",
       "   'Facilitation de processus',\n",
       "   'Design Thinking',\n",
       "   'Webdesign',\n",
       "   'User Research',\n",
       "   'Parcours utilisateur',\n",
       "   'User Testing',\n",
       "   'Figma',\n",
       "   'Prototype',\n",
       "   'Design System',\n",
       "   'Identité visuelle',\n",
       "   'Charte graphique',\n",
       "   'Branding',\n",
       "   'Graphisme',\n",
       "   'Design logo',\n",
       "   'Conception',\n",
       "   'Adobe Photoshop',\n",
       "   'Adobe Illustrator',\n",
       "   'Product Design',\n",
       "   'stratégie produit',\n",
       "   'Adobe InDesign',\n",
       "   'Prototypage rapide'],\n",
       "  'location': {'Localisation': 'Paris, France'},\n",
       "  'teletravail_preference': {'Peut travailler dans vos locaux à': 'Paris et 50km autour'},\n",
       "  'recommendations': 7,\n",
       "  'presentation': 'Créative, curieuse et rigoureuse, je suis passionnée par tout ce qui nourrit la culture digitale. Bénéficiant d’une forte expérience dans le graphisme print et web, je me suis spécialisée dans la conception d’expériences digitales centrées utilisateur. J’interviens selon vos besoins dès la phase de recherche utilisateur jusqu’à la conception visuelle de vos interfaces graphiques ainsi que les prototypes fonctionnels.MÉTHODOLOGIE DESIGN THINKING / AGILE :User Research : Étude de marché, Sondages, Interviews, Empathy Map, Étude ethnographique, Problem Statement…Définition du problème : How Might We, Personas, Story boards, User Flows, User journey…Idéation : Brainstorming, crazy 8, Sketches, Moscow, Affinity diagram…Prototypage : Wireframes, Architecture de l’information, Prototypes Low/Mid/High-fidelity, Design System…Tests : Usabilité, Désirabilité, A/B tests…BRANDING :identités visuelles / chartes graphiquesOUTILS :Figma, Invision, Suite Adobe (Indesign, Illustrator, Photoshop), Miro, Mural…'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE THE DATA TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e4291-0e0e-47c2-bf83-2913657d7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, csv_filename):\n",
    "    # Assurez-vous que la liste de données n'est pas vide\n",
    "    if not data:\n",
    "        print(\"Aucune donnée à enregistrer.\")\n",
    "        return\n",
    "\n",
    "    # Créez une liste de noms de colonnes pour le CSV en incluant les nouveaux éléments\n",
    "    fieldnames = ['name', 'headline', 'price', 'experience', 'response_rate', 'response_time', 'missions', 'categories', 'competences', 'supermalter', 'location','presentation', 'recommendations', 'teletravail_preference']\n",
    "\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Écrire les en-têtes\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Écrire les données\n",
    "        for entry in data:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "    print(f\"Données enregistrées dans {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce171a-cfba-4045-a89e-d83c0d4af58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données enregistrées dans malt_data.csv\n"
     ]
    }
   ],
   "source": [
    "csv_filename = 'malt_data.csv'\n",
    "save_to_csv(data, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
