{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83ed0194-fcec-4299-9ffd-cccc861119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import asyncio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE PROXY POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TEST PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5ad0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define custom options for the Selenium driver\n",
    "# options = Options()\n",
    "\n",
    "# options.add_argument(f'--proxy-server={proxy_pool[7]}')\n",
    "\n",
    "# # create the ChromeDriver instance with custom options\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.get('http://httpbin.org/ip')\n",
    "# # driver.get('https://www.malt.fr/profile/luccharlopeau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF GET USER TO FETCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets a df and gives the next profile to scrap, when its scrapped it updates the df\n",
    "def get_next_profile(df):\n",
    "    # get the first profile that is not scrapped\n",
    "    next_profile = df[df['scraped']==False].iloc[0]\n",
    "    # update the df\n",
    "    df.loc[df['profil']==next_profile['profil'], 'scraped'] = True\n",
    "    return df, next_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.DataFrame(columns=['name', 'headline', 'price', 'response_rate', 'response_time', 'categories', 'competences', 'supermalter', 'location','presentation', 'recommendations', 'teletravail_preference', 'profil', 'link', 'creation_date'])\n",
    "\n",
    "index_scrap = 0\n",
    "def add_to_df(data: dict): # save the data in a global df\n",
    "    global df_raw, index_scrap\n",
    "    \n",
    "    # Create a DataFrame with the current user's data\n",
    "    user_df = pd.DataFrame([data])  # Convert the user data to a DataFrame\n",
    "    \n",
    "    # Check if the user DataFrame has the same columns as df_raw\n",
    "    if user_df.columns.tolist() != df_raw.columns.tolist():\n",
    "        # If the columns don't match, ensure they align and reorder columns accordingly\n",
    "        user_df = user_df.reindex(columns=df_raw.columns)\n",
    "    \n",
    "    # Append the user DataFrame to df_raw\n",
    "    df_raw = pd.concat([df_raw, user_df], ignore_index=True)\n",
    "    \n",
    "    print(f'Scraped {index_scrap} users, last one is: {data[\"profil\"]}')\n",
    "    index_scrap += 1\n",
    "    \n",
    "    # Save the DataFrame to a CSV file every 1000 users\n",
    "    if index_scrap % 1000 == 0:\n",
    "        df_raw.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAP USER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2acab102-8494-4c6a-826b-db2e58992ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_user(row, driver):\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        driver.get(row['link'])\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    except:\n",
    "        print('error couldnt fetch the page')\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    price_element = soup.find('div', {'data-testid': 'profile-price'})\n",
    "    if price_element:\n",
    "        price = price_element.find('span', class_='block-list__price').text\n",
    "        data['price'] = price.strip()\n",
    "\n",
    "    # Récupérer l'expérience\n",
    "    experience_element = soup.find('span', string='Expérience')\n",
    "    if experience_element:\n",
    "        experience = experience_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['experience'] = experience.strip()\n",
    "    \n",
    "    # Récupérer le taux de réponse\n",
    "    response_rate_element = soup.find('span', string='Taux de réponse')\n",
    "    if response_rate_element:\n",
    "        response_rate = response_rate_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_rate'] = response_rate.strip()\n",
    "    \n",
    "    # Récupérer le temps de réponse\n",
    "    response_time_element = soup.find('span', string='Temps de réponse')\n",
    "    if response_time_element:\n",
    "        response_time = response_time_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_time'] = response_time.strip()\n",
    "        \n",
    "    # Récupérer le nom \n",
    "    name_element = soup.find('div', {'data-testid': 'profile-fullname'})\n",
    "    if name_element:\n",
    "        name = name_element.text\n",
    "        data['name'] = name.strip()\n",
    "        \n",
    "    # Récupérer le métier\n",
    "    headline_element = soup.find('div', {'data-testid': 'profile-headline'})\n",
    "    if headline_element:\n",
    "        headline = headline_element.text\n",
    "        data['headline'] = headline.strip()\n",
    "        \n",
    "    # Récupérer le nombre de missions\n",
    "    missions_element = soup.find('div', {'data-testid': 'profile-counter-missions'})\n",
    "    if missions_element:\n",
    "        missions = missions_element.find('strong').text\n",
    "        data['missions'] = missions.strip()\n",
    "        \n",
    "    # Récupérer toutes les catégories\n",
    "    categories_elements = soup.find_all('li', {'class': 'categories__list-item'})\n",
    "    categories = [category.find('a').text for category in categories_elements]\n",
    "    data['categories'] = categories\n",
    "    \n",
    "    # Récupérer les compétences\n",
    "    competences_element = soup.find_all('div', {'class': 'profile-expertises__content-list-item__label'})\n",
    "    competences = [competence.find('a', class_='joy-link joy-link_teal').text.strip() for competence in competences_element]\n",
    "\n",
    "    data['competences'] = competences\n",
    "    \n",
    "    # Récupérer le statut \"Supermalter\"\n",
    "    supermalter_element = soup.find('span', class_='joy-badge-level__tag blue')\n",
    "    if supermalter_element:\n",
    "        supermalter = supermalter_element.get_text(strip=True)\n",
    "        data['supermalter'] = supermalter\n",
    "        \n",
    "    # Récupérer la localisation\n",
    "    location_element = soup.find('dl', {'class': 'profile__location-and-workplace-preferences__item'})\n",
    "    if location_element:\n",
    "        location_label = location_element.find('dt', {'data-testid': 'profile-location-address-label'})\n",
    "        location_value = location_element.find('dd', {'data-testid': 'profile-location-preference-address'})\n",
    "\n",
    "        if location_label and location_value:\n",
    "            location = {location_label.text: location_value.text}\n",
    "            data['location'] = location\n",
    "            \n",
    "    # Récupérer la préférence de télétravail\n",
    "    teletravail_element = soup.find('dl', {'class': 'profile-page-mission-preferences__item'})\n",
    "    if teletravail_element:\n",
    "        teletravail_label = teletravail_element.find('dt')\n",
    "        teletravail_value = teletravail_element.find('dd')\n",
    "\n",
    "        if teletravail_label and teletravail_value:\n",
    "            teletravail_preference = {teletravail_label.text: teletravail_value.text}\n",
    "            data['teletravail_preference'] = teletravail_preference\n",
    "            \n",
    "    # Récupérer le nombre de recommandations\n",
    "    recommendations_element = soup.find('span', {'data-testid': 'profile-counter-recommendations'})\n",
    "    if recommendations_element:\n",
    "        recommendations_count = int(recommendations_element.text.split()[0])\n",
    "        data['recommendations'] = recommendations_count    \n",
    "\n",
    "    # Récupérer le message de présentation\n",
    "    presentation_element = soup.find('div', {'class': 'profile-description__content'})\n",
    "    if presentation_element:\n",
    "        presentation_message = presentation_element.get_text(strip=True)\n",
    "        data['presentation'] = presentation_message\n",
    "        \n",
    "    # add link of the profile\n",
    "    data['link'] = row['link']\n",
    "    \n",
    "    # add created date\n",
    "    data['creation_date'] = row['creation_date']\n",
    "    \n",
    "    # add name to the data\n",
    "    data['profil'] = row['profil']\n",
    "        \n",
    "    driver.quit() # close the browser\n",
    "\n",
    "    if data['location']:\n",
    "        add_to_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def configure_webdriver(proxy_address):\n",
    "    # define custom options for the Selenium driver\n",
    "    options = Options()\n",
    "\n",
    "    options.add_argument(f'--proxy-server={proxy_address}')\n",
    "    options.add_argument(\"window-size=400,200\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"enable-automation\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # create the ChromeDriver instance with custom options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    return driver\n",
    " \n",
    "def scrap_all_users_proxy(proxy_addresses, df):\n",
    "    index = 1\n",
    "    df_scraped = []\n",
    "    \n",
    "    while df[df['scraped'] == False].shape[0] > 0:  # While there are profiles to scrap\n",
    "        threads = []  # Store threads to manage them\n",
    "        \n",
    "        if len(proxy_addresses) > df[df['scraped'] == False].shape[0]: # If there are more proxies than profiles to scrap (to avoid length mismatch)\n",
    "            proxy_addresses = proxy_addresses[:df[df['scraped'] == False].shape[0]]\n",
    "\n",
    "        for proxy_address in proxy_addresses:\n",
    "            driver = configure_webdriver(proxy_address)\n",
    "            df, row = get_next_profile(df)\n",
    "\n",
    "            # Create a thread for each scraping task\n",
    "            thread = threading.Thread(target=scrap_user, args=(row, driver))\n",
    "            threads.append(thread)\n",
    "            thread.start()  # Start the thread\n",
    "        \n",
    "        # Wait for all threads to complete before proceeding\n",
    "        for thread in threads:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN THE SCRIPT WITH THE LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "703537a8-6bf8-44e5-bd74-0948d0c3ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 users, last one is: mickaelmolina\n",
      "Scraped 1 users, last one is: bricetillet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>headline</th>\n",
       "      <th>price</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>response_time</th>\n",
       "      <th>categories</th>\n",
       "      <th>competences</th>\n",
       "      <th>supermalter</th>\n",
       "      <th>location</th>\n",
       "      <th>presentation</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>teletravail_preference</th>\n",
       "      <th>profil</th>\n",
       "      <th>link</th>\n",
       "      <th>creation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mickael M.</td>\n",
       "      <td>Graphiste</td>\n",
       "      <td>250 €</td>\n",
       "      <td>100%</td>\n",
       "      <td>Quelques jours</td>\n",
       "      <td>[Graphiste]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Localisation': '21000 Dijon, France'}</td>\n",
       "      <td>Passionné depuis mon enfance par le graphisme,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Peut travailler dans vos locaux à': 'Dijon e...</td>\n",
       "      <td>mickaelmolina</td>\n",
       "      <td>https://www.malt.fr/profile/mickaelmolina</td>\n",
       "      <td>2013-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brice Tillet</td>\n",
       "      <td>Composer / Sound Designer / Music Producer</td>\n",
       "      <td>300 €</td>\n",
       "      <td>100%</td>\n",
       "      <td>12h</td>\n",
       "      <td>[Sound Designer]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Localisation': 'Paris, France'}</td>\n",
       "      <td>Bonjour,je suis musicien, compositeur et sound...</td>\n",
       "      <td>6</td>\n",
       "      <td>{'Peut travailler dans vos locaux à': 'Paris e...</td>\n",
       "      <td>bricetillet</td>\n",
       "      <td>https://www.malt.fr/profile/bricetillet</td>\n",
       "      <td>2014-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                                    headline  price  \\\n",
       "0    Mickael M.                                   Graphiste  250 €   \n",
       "1  Brice Tillet  Composer / Sound Designer / Music Producer  300 €   \n",
       "\n",
       "  response_rate   response_time        categories competences supermalter  \\\n",
       "0          100%  Quelques jours       [Graphiste]          []         NaN   \n",
       "1          100%             12h  [Sound Designer]          []         NaN   \n",
       "\n",
       "                                  location  \\\n",
       "0  {'Localisation': '21000 Dijon, France'}   \n",
       "1        {'Localisation': 'Paris, France'}   \n",
       "\n",
       "                                        presentation recommendations  \\\n",
       "0  Passionné depuis mon enfance par le graphisme,...             NaN   \n",
       "1  Bonjour,je suis musicien, compositeur et sound...               6   \n",
       "\n",
       "                              teletravail_preference         profil  \\\n",
       "0  {'Peut travailler dans vos locaux à': 'Dijon e...  mickaelmolina   \n",
       "1  {'Peut travailler dans vos locaux à': 'Paris e...    bricetillet   \n",
       "\n",
       "                                        link creation_date  \n",
       "0  https://www.malt.fr/profile/mickaelmolina    2013-08-02  \n",
       "1    https://www.malt.fr/profile/bricetillet    2014-01-08  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_links = pd.read_csv('../data/links.csv')\n",
    "profile_links['profil'] = profile_links['profil'].apply(lambda x: x.replace('https://www.malt.fr/profile/', ''))\n",
    "\n",
    "# add column link to the DataFrame\n",
    "profile_links['link'] = profile_links['profil'].apply(lambda x: f'https://www.malt.fr/profile/{x}')\n",
    "\n",
    "profile_links['scraped'] = False # add column scraped to the DataFrame\n",
    "\n",
    "# get first 10 rows of the DataFrame\n",
    "df = profile_links.iloc[:2]\n",
    "\n",
    "def main():\n",
    "    return scrap_all_users_proxy(proxy_pool, df)\n",
    "\n",
    "main()\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE THE DATA TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e61e4291-0e0e-47c2-bf83-2913657d7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, csv_filename):\n",
    "    # Assurez-vous que la liste de données n'est pas vide\n",
    "    if not data:\n",
    "        print(\"Aucune donnée à enregistrer.\")\n",
    "        return\n",
    "\n",
    "    # Créez une liste de noms de colonnes pour le CSV en incluant les nouveaux éléments\n",
    "    fieldnames = ['name', 'headline', 'price', 'experience', 'response_rate', 'response_time', 'missions', 'categories', 'competences', 'supermalter', 'location','presentation', 'recommendations', 'teletravail_preference']\n",
    "\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Écrire les en-têtes\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Écrire les données\n",
    "        for entry in data:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "    print(f\"Données enregistrées dans {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c6ce171a-cfba-4045-a89e-d83c0d4af58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune donnée à enregistrer.\n"
     ]
    }
   ],
   "source": [
    "csv_filename = 'malt_data.csv'\n",
    "save_to_csv(data, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
