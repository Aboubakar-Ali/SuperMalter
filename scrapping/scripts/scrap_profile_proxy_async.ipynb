{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83ed0194-fcec-4299-9ffd-cccc861119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import asyncio\n",
    "import time\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE PROXY POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE TEST PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5ad0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define custom options for the Selenium driver\n",
    "# options = Options()\n",
    "\n",
    "# options.add_argument(f'--proxy-server={proxy_pool[7]}')\n",
    "\n",
    "# # create the ChromeDriver instance with custom options\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.get('http://httpbin.org/ip')\n",
    "# # driver.get('https://www.malt.fr/profile/luccharlopeau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKUP CSV CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_dataframe_periodically(df_scraped):\n",
    "    while True:\n",
    "        await asyncio.sleep(3600)  # Sleep for one hour (3600 seconds)\n",
    "        df_scraped.to_csv('scraped_data.csv', index=False)  # Save the DataFrame to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets a df and gives the next profile to scrap, when its scrapped it updates the df\n",
    "def get_next_profile(df):\n",
    "    # get the first profile that is not scrapped\n",
    "    next_profile = df[df['scraped']==False].iloc[0]\n",
    "    # update the df\n",
    "    df.loc[df['profil']==next_profile['profil'], 'scraped'] = True\n",
    "    return df, next_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAP USER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2acab102-8494-4c6a-826b-db2e58992ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_user(row, driver):\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "    driver.get(row['link'])\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    price_element = soup.find('div', {'data-testid': 'profile-price'})\n",
    "    if price_element:\n",
    "        price = price_element.find('span', class_='block-list__price').text\n",
    "        data['price'] = price.strip()\n",
    "    \n",
    "    # Récupérer l'expérience\n",
    "    experience_element = soup.find('span', string='Expérience')\n",
    "    if experience_element:\n",
    "        experience = experience_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['experience'] = experience.strip()\n",
    "    \n",
    "    # Récupérer le taux de réponse\n",
    "    response_rate_element = soup.find('span', string='Taux de réponse')\n",
    "    if response_rate_element:\n",
    "        response_rate = response_rate_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_rate'] = response_rate.strip()\n",
    "    \n",
    "    # Récupérer le temps de réponse\n",
    "    response_time_element = soup.find('span', string='Temps de réponse')\n",
    "    if response_time_element:\n",
    "        response_time = response_time_element.find_next('span', class_='profile-indicators-content').text\n",
    "        data['response_time'] = response_time.strip()\n",
    "        \n",
    "    # Récupérer le nom \n",
    "    name_element = soup.find('div', {'data-testid': 'profile-fullname'})\n",
    "    if name_element:\n",
    "        name = name_element.text\n",
    "        data['name'] = name.strip()\n",
    "        \n",
    "    # Récupérer le métier\n",
    "    headline_element = soup.find('div', {'data-testid': 'profile-headline'})\n",
    "    if headline_element:\n",
    "        headline = headline_element.text\n",
    "        data['headline'] = headline.strip()\n",
    "        \n",
    "    # Récupérer le nombre de missions\n",
    "    missions_element = soup.find('div', {'data-testid': 'profile-counter-missions'})\n",
    "    if missions_element:\n",
    "        missions = missions_element.find('strong').text\n",
    "        data['missions'] = missions.strip()\n",
    "        \n",
    "    # Récupérer toutes les catégories\n",
    "    categories_elements = soup.find_all('li', {'class': 'categories__list-item'})\n",
    "    categories = [category.find('a').text for category in categories_elements]\n",
    "    data['categories'] = categories\n",
    "    \n",
    "    \n",
    "    # Récupérer les compétences\n",
    "    competences_element = soup.find_all('div', {'class': 'profile-expertises__content-list-item__label'})\n",
    "    competences = [competence.find('a', class_='joy-link joy-link_teal').text.strip() for competence in competences_element]\n",
    "\n",
    "    data['competences'] = competences\n",
    "    \n",
    "    # Récupérer le statut \"Supermalter\"\n",
    "    supermalter_element = soup.find('span', class_='joy-badge-level__tag blue')\n",
    "    if supermalter_element:\n",
    "        supermalter = supermalter_element.get_text(strip=True)\n",
    "        data['supermalter'] = supermalter\n",
    "        \n",
    "    # Récupérer la localisation\n",
    "    location_element = soup.find('dl', {'class': 'profile__location-and-workplace-preferences__item'})\n",
    "    if location_element:\n",
    "        location_label = location_element.find('dt', {'data-testid': 'profile-location-address-label'})\n",
    "        location_value = location_element.find('dd', {'data-testid': 'profile-location-preference-address'})\n",
    "\n",
    "        if location_label and location_value:\n",
    "            location = {location_label.text: location_value.text}\n",
    "            data['location'] = location\n",
    "            \n",
    "    # Récupérer la préférence de télétravail\n",
    "    teletravail_element = soup.find('dl', {'class': 'profile-page-mission-preferences__item'})\n",
    "    if teletravail_element:\n",
    "        teletravail_label = teletravail_element.find('dt')\n",
    "        teletravail_value = teletravail_element.find('dd')\n",
    "\n",
    "        if teletravail_label and teletravail_value:\n",
    "            teletravail_preference = {teletravail_label.text: teletravail_value.text}\n",
    "            data['teletravail_preference'] = teletravail_preference\n",
    "            \n",
    "    # Récupérer le nombre de recommandations\n",
    "    recommendations_element = soup.find('span', {'data-testid': 'profile-counter-recommendations'})\n",
    "    if recommendations_element:\n",
    "        recommendations_count = int(recommendations_element.text.split()[0])\n",
    "        data['recommendations'] = recommendations_count   \n",
    "        \n",
    "\n",
    "    # Récupérer le message de présentation\n",
    "    presentation_element = soup.find('div', {'class': 'profile-description__content'})\n",
    "    if presentation_element:\n",
    "        presentation_message = presentation_element.get_text(strip=True)\n",
    "        data['presentation'] = presentation_message\n",
    "        \n",
    "    # add link of the profile\n",
    "    data['link'] = row['link']\n",
    "    \n",
    "    # add created date\n",
    "    data['creation_date'] = row['creation_date']\n",
    "    \n",
    "    # add name to the data\n",
    "    data['profil'] = row['profil']\n",
    "        \n",
    "    driver.quit() # close the browser\n",
    "    # time.sleep(5) # wait for 5 seconds to avoid getting banned\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    return data # return the data scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from selenium import webdriver\n",
    "import threading\n",
    "\n",
    "async def configure_webdriver(proxy_address):\n",
    "    # define custom options for the Selenium driver\n",
    "    options = Options()\n",
    "\n",
    "    options.add_argument(f'--proxy-server={proxy_address}')\n",
    "    # options.add_argument(\"--headless\") # breaks the code\n",
    "    options.add_argument(\"window-size=400,200\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"enable-automation\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    # create the ChromeDriver instance with custom options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "async def scrap_all_users_proxy(proxy_addresses, df):\n",
    "    tasks = []\n",
    "    index = 1\n",
    "    df_scraped = pd.DataFrame()\n",
    "    \n",
    "    # # save the df every hour\n",
    "    # loop = asyncio.get_event_loop()\n",
    "    # loop.create_task(save_dataframe_periodically(df_scraped))\n",
    "    \n",
    "    while df[df['scraped']==False].shape[0] > 0: # while there are profiles to scrap\n",
    "        for proxy_address in proxy_addresses: # for each proxy address\n",
    "            driver = await configure_webdriver(proxy_address) # configure the driver with the proxy address\n",
    "            df, row = get_next_profile(df) # get the next profile to scrap\n",
    "            task = asyncio.create_task(scrap_user(row, driver)) # scrap the profile in a task (async) and save the result in a variable \n",
    "            tasks.append(task)\n",
    "            \n",
    "        results = await asyncio.gather(*tasks) # wait for all the tasks to finish\n",
    "        \n",
    "        \n",
    "\n",
    "        print(results)\n",
    "        print(f'Fetched ', index, 'to', index+10, ' profiles')\n",
    "        index += 10\n",
    "        \n",
    "        # update the df\n",
    "        \n",
    "        #save every hour\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN THE SCRIPT WITH THE LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "703537a8-6bf8-44e5-bd74-0948d0c3ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categories': [], 'competences': [], 'link': 'https://www.malt.fr/profile/bricetillet', 'creation_date': '2014-01-08', 'profil': 'bricetillet'}\n",
      "{'categories': [], 'competences': [], 'link': 'https://www.malt.fr/profile/mickaelmolina', 'creation_date': '2013-08-02', 'profil': 'mickaelmolina'}\n",
      "{'price': '700\\xa0€', 'experience': '-', 'response_rate': '100%', 'response_time': '1h', 'name': 'Alexandre Labayle', 'headline': 'Consultant décisionnel Senior', 'categories': [], 'competences': [], 'location': {'Localisation': ''}, 'teletravail_preference': {'Peut travailler dans vos locaux à': ' et 50km autour'}, 'presentation': \"Consultant Senior en Business Intelligence , 14 ans d'expérience.Certifié SAP Business Objects et Microsoft BI. Missions d'architecture technique (cluster SAP BI ,implémentation de sécurité). Modélisation et alimentation de DatawareHouse (Dataservices / SSIS/SSAS) . Création de rapports Excel PowerPivot/Sap Business Objects .Création d'univers ou de cubes SSAS.\", 'link': 'https://www.malt.fr/profile/alexandrelabayle', 'creation_date': '2014-02-04', 'profil': 'alexandrelabayle'}\n",
      "{'price': '40\\xa0€', 'experience': '-', 'response_time': '1h', 'name': 'Nacera  TIZI', 'headline': 'Commerciale Freelance', 'categories': ['Business developers'], 'competences': [], 'location': {'Localisation': 'Tanger, Tanger-Tétouan, Maroc'}, 'teletravail_preference': {'Peut travailler dans vos locaux à': 'Tanger et 50km autour'}, 'presentation': \"Parce que La ville de Tanger tend à devenir l'une des plus importantes au Maroc. Placée deuxième ville économique je suis aujourd'hui en mesure de développer votre chiffre d'affaire au Maroc.Diplômée d’un Master 2 Économie gestion des entreprises, spécialité contrôle de gestion Audit à l'université de Lille 1 en France. J’ai eu l’occasion d’effectuer plusieurs jobs dans le domaine du commerce, ainsi faites moi confiance.\", 'link': 'https://www.malt.fr/profile/naceratizi', 'creation_date': '2014-02-22', 'profil': 'naceratizi'}\n",
      "{'price': '550\\xa0€', 'experience': '8-15 ans', 'response_rate': '100%', 'response_time': '1h', 'name': 'Aurélien D.', 'headline': 'SysOps, DevOps et développeur web', 'missions': '12', 'categories': ['Administrateur base de données', 'Administrateur Système et Réseaux', 'Développeur CMS', 'Développeur ERP', 'Développeur Web Back-end', 'Développeur Web Front-end', 'DevOps', 'Intégrateur Web'], 'competences': [], 'location': {'Localisation': 'La Rochelle, France'}, 'teletravail_preference': {'Télétravail': 'Effectue ses missions majoritairement à distance'}, 'recommendations': 1, 'presentation': 'SysOps (unix) depuis 5 ans et DevOps depuis 4 ans, j\\'ai également une expérience de plus de 12 ans en tant que développeur web full stack.En tant qu\\'administrateur système Unix expérimenté, je peux vous aider à gérer et à optimiser votre serveur Unix. Que ce soit pour configurer un nouveau serveur, résoudre des problèmes de performance ou de sécurité, ou automatiser des tâches répétitives, je peux vous offrir une assistance professionnelle pour assurer le bon fonctionnement de votre système.Les services que je peux fournir incluent :- Installation et configuration de système d\\'exploitation (Unix / Linux uniquement)- Configuration et gestion des services réseau (comme SSH, FTP, DNS)- Gestion des utilisateurs et des groupes- Sauvegarde et restauration de données- Mise en place de stratégies de sécurité- Optimisation des performances- Automatisation des tâches à l\\'aide de scripts (bash, python, etc.)- Diagnostique et résolution des problèmesJe m\\'efforce de fournir un service rapide et efficace, avec une communication claire et régulière pour vous tenir informé de l\\'avancée de la mission.Au niveau du développement web j\\'aime créer des solutions clé en main \"sur mesure\", sans forcément partir d\\'un existant. Mais je sais aussi bien reprendre un projet déjà développé afin de le maintenir ou l\\'améliorer.Je souhaite apporter mon expertise et mes compétences au service de projets divers et variés mais également confirmer voire élargir mes connaissances.N\\'hésitez pas à me contacter pour une première discussion de votre projet, je suis joignable par téléphone, malt, email ou tout simplement autour d\\'un café :).À bientôt !', 'link': 'https://www.malt.fr/profile/aureliendazy', 'creation_date': '2013-06-06', 'profil': 'aureliendazy'}\n",
      "{'price': '235\\xa0€', 'experience': '-', 'response_rate': '33%', 'response_time': '12h', 'name': 'Youri Galescot', 'headline': 'Développeur web frontend et backend', 'categories': [], 'competences': [], 'location': {'Localisation': 'Paris, France'}, 'teletravail_preference': {'Peut travailler dans vos locaux à': 'Paris et 50km autour'}, 'presentation': \"Bonjour,Je suis développeur frontend et backend freelance depuis 2012.Diplômé d'un master en systèmes d'information je maîtrise bien le design et le développement de sites web, domaine qui me passionne. Je peux créer aussi bien des sites web vitrines que des sites web dynamiques, dotés d’un CMS.Je code en HTML5/CSS3, PHP, jQuery, sur des frameworks différents comme CodeIgniter, Symfony 2, sur le CMS Wordpress ainsi que sur Bootstrap.J'ai créé plusieurs sites web dans le cadre de mon activité d'autoentrepreneur.Mes sites sont optimisés pour les supports mobiles et pour le référencement naturel.Si vous êtes intéressés par mon profil, n'hésitez pas à me contacter.\", 'link': 'https://www.malt.fr/profile/yourigalescot', 'creation_date': '2014-02-18', 'profil': 'yourigalescot'}\n",
      "{'price': '300\\xa0€', 'experience': '-', 'response_rate': '100%', 'response_time': 'Quelques jours', 'name': 'Siobhan Engelmann', 'headline': 'Traduction Translation', 'categories': ['Traducteur'], 'competences': [], 'location': {'Localisation': 'Metz, France'}, 'teletravail_preference': {'Peut travailler dans vos locaux à': 'Metz et 50km autour'}, 'presentation': 'English native speaker, degree qualified engineer, translate technical / engineering  texts from French, German and Italian into English.  All formats accepted including http, Quick turnaround.  15cents / word', 'link': 'https://www.malt.fr/profile/siobhanengelmann', 'creation_date': '2014-02-11', 'profil': 'siobhanengelmann'}\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_TUNNEL_CONNECTION_FAILED\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n#0 0x5572a51e76d4 <unknown>\n#1 0x5572a4eec48e <unknown>\n#2 0x5572a4ee59e9 <unknown>\n#3 0x5572a4ed8602 <unknown>\n#4 0x5572a4ed9d05 <unknown>\n#5 0x5572a4ed8b88 <unknown>\n#6 0x5572a4ed7999 <unknown>\n#7 0x5572a4ed77ff <unknown>\n#8 0x5572a4ed65ba <unknown>\n#9 0x5572a4ed6a15 <unknown>\n#10 0x5572a4eef265 <unknown>\n#11 0x5572a4f6b441 <unknown>\n#12 0x5572a4f54132 <unknown>\n#13 0x5572a4f6af65 <unknown>\n#14 0x5572a4f53ed3 <unknown>\n#15 0x5572a4f26420 <unknown>\n#16 0x5572a4f27a93 <unknown>\n#17 0x5572a51ba4c0 <unknown>\n#18 0x5572a51bd780 <unknown>\n#19 0x5572a51bd1fa <unknown>\n#20 0x5572a51bdc95 <unknown>\n#21 0x5572a51ac65b <unknown>\n#22 0x5572a51be080 <unknown>\n#23 0x5572a5197830 <unknown>\n#24 0x5572a51d7ee7 <unknown>\n#25 0x5572a51d80f5 <unknown>\n#26 0x5572a51e6cce <unknown>\n#27 0x7f3d586aa9eb <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m scrap_all_users_proxy(proxy_pool, df)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m main()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m data\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m scrap_all_users_proxy(proxy_pool, df)\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mcreate_task(scrap_user(row, driver)) \u001b[39m# scrap the profile in a task (async) and save the result in a variable \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     tasks\u001b[39m.\u001b[39mappend(task)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks) \u001b[39m# wait for all the tasks to finish\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(results)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFetched \u001b[39m\u001b[39m'\u001b[39m, index, \u001b[39m'\u001b[39m\u001b[39mto\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m profiles\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mscrap_user\u001b[39m(row, driver):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     wait \u001b[39m=\u001b[39m WebDriverWait(driver, \u001b[39m5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     driver\u001b[39m.\u001b[39;49mget(row[\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     page_source \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mpage_source\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lux/Documents/ScrapMalt/scrapping/scripts/scrap_profile_proxy_async.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(page_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    348\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/ScrapMalt/.venv/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_TUNNEL_CONNECTION_FAILED\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n#0 0x5572a51e76d4 <unknown>\n#1 0x5572a4eec48e <unknown>\n#2 0x5572a4ee59e9 <unknown>\n#3 0x5572a4ed8602 <unknown>\n#4 0x5572a4ed9d05 <unknown>\n#5 0x5572a4ed8b88 <unknown>\n#6 0x5572a4ed7999 <unknown>\n#7 0x5572a4ed77ff <unknown>\n#8 0x5572a4ed65ba <unknown>\n#9 0x5572a4ed6a15 <unknown>\n#10 0x5572a4eef265 <unknown>\n#11 0x5572a4f6b441 <unknown>\n#12 0x5572a4f54132 <unknown>\n#13 0x5572a4f6af65 <unknown>\n#14 0x5572a4f53ed3 <unknown>\n#15 0x5572a4f26420 <unknown>\n#16 0x5572a4f27a93 <unknown>\n#17 0x5572a51ba4c0 <unknown>\n#18 0x5572a51bd780 <unknown>\n#19 0x5572a51bd1fa <unknown>\n#20 0x5572a51bdc95 <unknown>\n#21 0x5572a51ac65b <unknown>\n#22 0x5572a51be080 <unknown>\n#23 0x5572a5197830 <unknown>\n#24 0x5572a51d7ee7 <unknown>\n#25 0x5572a51d80f5 <unknown>\n#26 0x5572a51e6cce <unknown>\n#27 0x7f3d586aa9eb <unknown>\n"
     ]
    }
   ],
   "source": [
    "profile_links = pd.read_csv('../data/links.csv')\n",
    "profile_links['profil'] = profile_links['profil'].apply(lambda x: x.replace('https://www.malt.fr/profile/', ''))\n",
    "\n",
    "# add column link to the DataFrame\n",
    "profile_links['link'] = profile_links['profil'].apply(lambda x: f'https://www.malt.fr/profile/{x}')\n",
    "\n",
    "profile_links['scraped'] = False # add column scraped to the DataFrame\n",
    "\n",
    "# get first 10 rows of the DataFrame\n",
    "df = profile_links.iloc[:30]\n",
    "\n",
    "async def main():\n",
    "    return await scrap_all_users_proxy(proxy_pool, df)\n",
    "\n",
    "data = await main()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE THE DATA TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e4291-0e0e-47c2-bf83-2913657d7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, csv_filename):\n",
    "    # Assurez-vous que la liste de données n'est pas vide\n",
    "    if not data:\n",
    "        print(\"Aucune donnée à enregistrer.\")\n",
    "        return\n",
    "\n",
    "    # Créez une liste de noms de colonnes pour le CSV en incluant les nouveaux éléments\n",
    "    fieldnames = ['name', 'headline', 'price', 'experience', 'response_rate', 'response_time', 'missions', 'categories', 'competences', 'supermalter', 'location','presentation', 'recommendations', 'teletravail_preference']\n",
    "\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        # Écrire les en-têtes\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Écrire les données\n",
    "        for entry in data:\n",
    "            writer.writerow(entry)\n",
    "\n",
    "    print(f\"Données enregistrées dans {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce171a-cfba-4045-a89e-d83c0d4af58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune donnée à enregistrer.\n"
     ]
    }
   ],
   "source": [
    "csv_filename = 'malt_data.csv'\n",
    "save_to_csv(data, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
